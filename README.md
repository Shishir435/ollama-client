# ğŸ§  Ollama Client â€” Chat with Local LLMs in Your Browser

**Ollama Client** is a powerful yet lightweight Chrome extension that lets you interact with locally hosted LLMs using [Ollama](https://ollama.com). Perfect for developers, researchers, and power users who want fast, private AI responses directly inside their browser.

---

### ğŸš€ Get Started â€” Install Now!

<div style="text-align:center; margin: 20px 0;">
  <a href="https://chromewebstore.google.com/detail/ollama-client/bfaoaaogfcgomkjfbmfepbiijmciinjl" target="_blank" 
     style="background-color:#4F46E5; color:white; font-size:18px; font-weight:bold; padding: 15px 40px; border-radius:8px; text-decoration:none; display:inline-block;">
    ğŸš€ Install Ollama Client from Chrome Web Store
  </a>
</div>

---

## ğŸŒ Explore More

Check out the official landing page with full documentation and guides:
ğŸ‘‰ [ollama-client](https://shishir435.github.io/ollama-client/ollama-client)

---

## ğŸš€ Key Features

- ğŸ”Œ **Local Ollama Integration** â€“ Connect to your own Ollama server, no API keys required.
- ğŸ’¬ **In-Browser Chat UI** â€“ Lightweight, minimal chat interface.
- âš™ï¸ **Custom Settings Panel** â€“ Configure base URL, default model, themes, excluded URLs, and prompt templates.
- ğŸ”„ **Model Switcher** â€“ Switch between any installed Ollama models on the fly.
- ğŸ§­ **Model Search & Add** â€“ Search, pull, and add new Ollama models and track download progress directly from the options page. _(Known issue: pressing Stop during model pull may cause some glitches.)_
- ğŸ›ï¸ **Model Parameter Tuning** â€“ Adjust temperature, top_k, top_p, repeat penalty and stop sequence.
- âœ‚ï¸ **Content Parsing** â€“ Automatically extract and summarize page content with Mozilla Readability.
- ğŸ“œ **Transcript Parsing** â€“ Supports transcripts from YouTube, Udemy, Coursera.
- ğŸ”Š **Text-to-Speech** â€“ Click the â€œSpeakâ€ button to have the browser read aloud chat responses or page summaries using the Web Speech API.
- ğŸ“‹ **Regenerate / Copy Response** â€“ Easily rerun AI responses or copy results to clipboard.
- ğŸ—‚ï¸ **Multi-Chat Sessions** â€“ Manage multiple chat sessions locally with save, load, and delete.
- ğŸ›¡ï¸ **Privacy-First** â€“ All data processing and storage stays local on your machine.
- ğŸ§¯ **Declarative Net Request (DNR)** â€“ Handles CORS automatically, no manual config needed (since v0.1.3).

---

## ğŸ› ï¸ Quick Installation Guide

### âœ… 1. Install the Chrome Extension

Get it from the Chrome Web Store:
ğŸ‘‰ [Ollama Client - Chrome Extension](https://chromewebstore.google.com/detail/ollama-client/bfaoaaogfcgomkjfbmfepbiijmciinjl)

---

### âœ… 2. Install Ollama on Your System

Visit: [https://ollama.com](https://ollama.com)
Then run:

```bash
ollama serve
```

This starts the local server at `http://localhost:11434`.

---

### âœ… 3. Pull a Model (e.g., Gemma 3B)

```bash
ollama pull gemma3:1b
```

> You can also pull other models like `llama3:8b`, `mistral`, `codellama`, etc.

---

### âš™ï¸ 4. Configure CORS for Chrome Extension Access (If Needed)

Since v0.1.3, Ollama Client uses Chrome's **Declarative Net Request** API to bypass CORS automatically in most environments.
If you still encounter:

> âŒ 403 Forbidden: CORS Error

Please follow this detailed setup:
ğŸ“– [Ollama Setup Guide](https://shishir435.github.io/ollama-client/ollama-setup-guide)

---

### âš™ï¸ Configuration & Options

After installing:

1. Click the **Ollama Client** icon in your browser.
2. Open the âš™ï¸ **Settings Page**.
3. Configure:

   - âœ… Base URL (`http://localhost:11434`)
   - ğŸ¤– Default Model (e.g., `gemma:3b`)
   - ğŸ¨ Theme Preferences
   - ğŸš« Excluded URLs (for auto-context)
   - ğŸ“Œ Prompt Templates (prefilled tasks like summarize, translate, explain)
   - ğŸ”§ **Model Parameters**:

     - ğŸ”¥ **Temperature** (e.g., `0.87`)
     - ğŸ¯ **Top-K** (e.g., `37`)
     - ğŸ“ˆ **Top-P** (e.g., `0.6`)
     - ğŸ§  **Repeat Penalty** (e.g., `1.1`)
     - ğŸ§¾ **System Prompt** (e.g., `You are a helpful assistant...`)
     - ğŸ›‘ **Stop Sequences** (e.g., custom output termination strings)

> These settings are saved per model and can be adjusted anytime via the options menu.

---

## ğŸ¤” Which Ollama Model Should You Use?

| System Specs                         | Recommended Models           | Notes                                                               |
| ------------------------------------ | ---------------------------- | ------------------------------------------------------------------- |
| ğŸ”¹ **8GB RAM** (no GPU)              | `gemma:2b`, `mistral:7b-q4`  | Prefer small quantized models (e.g., `q4_0`) for smooth performance |
| ğŸ”¹ **16GB RAM** (no GPU)             | `gemma:2b`, `gemma:3b-q4`    | Avoid large models; quantized recommended                           |
| ğŸ”¹ **16GB+ RAM** with GPU (6GB VRAM) | `gemma:3b`, `llama3:8b-q4`   | Still use quantized models for efficiency                           |
| ğŸ”¹ **32GB+ RAM** or high-end GPU     | `llama3:8b`, `codellama:13b` | Can run larger models with better speed                             |
| ğŸ”¹ **RTX 3090+ / Apple M3 Max**      | `llama3:70b`, `mixtral`      | For high-end machines only due to resource demands                  |

> âœ… **Tip:** Quantized models (e.g., `gemma:3b-q4_0`) are preferred for better compatibility and performance.

ğŸ“š **Browse More Models**:
ğŸ‘‰ [Ollama Model Library](https://ollama.com/library)

---

## ğŸ§© Tech Stack

- TypeScript
- React + Vite
- Shadcn Ui
- Chrome Extension APIs (Declarative Net Request)
- Ollama (local LLM backend)

---

## ğŸ Known Issues and Limitations

- ğŸš« **Stop Generation Bug:** "Stop" only works after generation starts streaming; sometimes it doesnâ€™t abort early.
- ğŸ¤– **Stop pulling model bug** pressing "Stop" during model pull may cause some glitches.
- ğŸ›‘ **CORS Issues:** Mostly resolved via DNR, but manual CORS setup may still be needed in some environments.
- ğŸ’¾ **Storage Size:** IndexedDB storage is local; large chat histories may affect performance on low-end devices.

---

## ğŸ”® Upcoming Features

- Enhanced chat history management with search and export options.
- Improved error handling and user notifications.

---

## ğŸ”— Useful Links

- ğŸŒ **Install Extension:** [Chrome Web Store](https://chromewebstore.google.com/detail/ollama-client/bfaoaaogfcgomkjfbmfepbiijmciinjl)
- ğŸš€ **Landing Page** [ollama client](https://shishir435.github.io/ollama-client/ollama-client)
- ğŸ“– **Setup Guide:** [Ollama Setup Instructions](https://shishir435.github.io/ollama-client/ollama-setup-guide)
- ğŸ’» **GitHub Repo:** [github.com/Shishir435/ollama-client](https://github.com/Shishir435/ollama-client)
- ğŸ› **Issue Tracker:** [Report a Bug](https://github.com/Shishir435/ollama-client/issues)
- ğŸ™‹â€â™‚ï¸ **Portfolio:** [shishirchaurasiya.in](https://www.shishirchaurasiya.in)
- ğŸ’Œ **Feature request** [email](mailto:shishirchaurasiya435@gmail.com)

---
