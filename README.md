---
# ğŸ§  Ollama Client â€” Chat with Local LLMs in Your Browser

**Ollama Client** is a powerful yet lightweight Chrome extension that lets you interact with locally hosted LLMs using [Ollama](https://ollama.com). Perfect for developers, researchers, and power users who want fast, private AI responses directly inside their browser.

> âœ… Now available on the Chrome Web Store: [Install Ollama Client](https://chromewebstore.google.com/detail/ollama-client/bfaoaaogfcgomkjfbmfepbiijmciinjl)

---

## ğŸš€ Key Features

- ğŸ”Œ **Local Ollama Integration** â€“ Connect to your own Ollama server, no API keys required.
- ğŸ’¬ **In-Browser Chat UI** â€“ Lightweight, minimal chat interface.
- ğŸ”„ **Model Switcher** â€“ Choose from any installed Ollama model on the fly.
- âš™ï¸ **Custom Settings Panel** â€“ Configure base URL, default model, theme, and excluded URLs.
- âœ‚ï¸ **Content Parsing** â€“ Automatically reads and summarizes page content using Mozilla Readability.
- ğŸ“œ **Transcript Parsing** â€“ Extracts transcripts from supported platforms like YouTube, Udemy, and Coursera.
- ğŸ“‹ **Regenerate / Copy Response** â€“ Rerun answers and easily copy outputs.

---

## ğŸ§° How to Set Up Ollama for This Extension

To use this extension, you need to:

1. **Install Ollama on your machine**
2. **Pull a model** (e.g., `gemma:3b` or `llama3:8b`)
3. **Allow Chrome Extensions via CORS**

Follow this step-by-step guide:
ğŸ“– [Ollama Setup Guide for Browser Extensions](https://shishir435.github.io/ollama-client/ollama-setup-guide)

---

## ğŸ› ï¸ Quick Installation Guide

### âœ… 1. Install the Chrome Extension

Get it from the Chrome Web Store:
ğŸ‘‰ [Ollama Client - Chrome Extension](https://chromewebstore.google.com/detail/ollama-client/bfaoaaogfcgomkjfbmfepbiijmciinjl)

---

### âœ… 2. Install Ollama on Your System

Visit: [https://ollama.com](https://ollama.com)
Then run:

```bash
ollama serve
```

This starts a local server at `http://localhost:11434`.

---

### âœ… 3. Pull a Model (e.g., Gemma 3B)

```bash
ollama pull gemma3:1b
```

> You can also pull other models like `llama3:8b`, `mistral`, `codellama`, etc.

---

### âš™ï¸ 4. Configure CORS for Chrome Extension Access

If you see this error:

> âŒ 403 Forbidden: CORS Error
> Your Ollama server is blocking requests from this Chrome extension.

Then follow the full instructions here:
ğŸ“– [Ollama Setup Guide](https://shishir435.github.io/ollama-client/ollama-setup-guide)

It includes specific steps for:

- ğŸ–¥ï¸ macOS (Launch Agent)
- ğŸ§ Linux (systemd)
- ğŸªŸ Windows (Environment Variables)

Example config:

```bash
export OLLAMA_ORIGINS=chrome-extension://*
```

---

## âš™ï¸ Configuration & Options

After installing:

1. Click the **Ollama Client** icon in your browser.
2. Open the âš™ï¸ **Settings Page**.
3. Configure:

   - âœ… Base URL (`http://localhost:11434`)
   - ğŸ¤– Default Model (e.g., `gemma:3b`)
   - ğŸ¨ Theme Preferences
   - ğŸš« Excluded URLs (for auto-context)

---

### ğŸ¤” Which Ollama Model Should You Use?

Not sure what model your machine can handle? Here's a quick guide based on your hardware:

| System Specs                         | Recommended Models           | Notes                                                                |
| ------------------------------------ | ---------------------------- | -------------------------------------------------------------------- |
| ğŸ”¹ **8GB RAM** (no GPU)              | `gemma:2b`, `mistral:7b-q4`  | Stick to small **quantized** models (e.g., `q4_0`)                   |
| ğŸ”¹ **16GB RAM** (no GPU)             | `gemma:2b`, `gemma:3b-q4`    | Avoid anything above 3B. Use quantized models for better performance |
| ğŸ”¹ **16GB+ RAM** with GPU (6GB VRAM) | `gemma:3b`, `llama3:8b-q4`   | Still use quantized models. Avoid full-precision large models        |
| ğŸ”¹ **32GB+ RAM** or **high-end GPU** | `llama3:8b`, `codellama:13b` | Can run larger models with better speed and quality                  |
| ğŸ”¹ **RTX 3090+ / Apple M3 Max**      | `llama3:70b`, `mixtral`      | These are massive models â€” only use on very high-end machines        |

> âœ… **Tip:** Always prefer quantized models (e.g., `gemma:3b-q4_0`) for better compatibility on low-memory systems.

ğŸ“š **Browse More Models**:
ğŸ‘‰ [Ollama Model Library](https://ollama.com/library)

---

## ğŸ§© Tech Stack

- **TypeScript**
- **React + Vite**
- **TailwindCSS**
- **Lucide Icons**
- **Chrome Extension APIs**
- **Ollama (LLM backend)**

---

## ğŸ”— Useful Links

- ğŸŒ **Install Extension**: [Chrome Web Store](https://chromewebstore.google.com/detail/ollama-client/bfaoaaogfcgomkjfbmfepbiijmciinjl)
- ğŸ“– **Setup Guide**: [Ollama Setup Instructions](https://shishir435.github.io/ollama-client/ollama-setup-guide)
- ğŸ’» **GitHub Repo**: [github.com/Shishir435/ollama-client](https://github.com/Shishir435/ollama-client)
- ğŸ› **Issue Tracker**: [Report a Bug](https://github.com/Shishir435/ollama-client/issues)
- ğŸ™‹â€â™‚ï¸ **Portfolio**: [shishirchaurasiya.in](https://www.shishirchaurasiya.in)

---
